{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-WttHYyhXuz"
      },
      "source": [
        "## Sixth Session (Related to the Course Project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgciSSBhXu0"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5qKxrjhXu0"
      },
      "source": [
        "## Graph Classification with [Deep Graph Library (DGL)](https://docs.dgl.ai/index.html) for the graduate course \"[Graph Machine learning](https://github.com/zahta/graph_ml)\"\n",
        "\n",
        "### Dataset: bbbp\n",
        "\n",
        "##### by [Zahra Taheri](https://github.com/zahta), 06 June 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77T5Lv2hhXu0"
      },
      "source": [
        "---------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoTzK-2IhXu1"
      },
      "source": [
        "### This Tutorial Is Prepared Based on the Following References\n",
        "\n",
        "- [FunQG: Molecular Representation Learning via Quotient Graphs](https://pubs.acs.org/doi/10.1021/acs.jcim.3c00445)\n",
        "- [Supporting Information of FunQG](https://pubs.acs.org/doi/suppl/10.1021/acs.jcim.3c00445/suppl_file/ci3c00445_si_001.pdf)\n",
        "- [GitHub Repository of FunQG](https://github.com/hhaji/funqg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5IQfzqdbMY4",
        "outputId": "b83c0867-fd49-4202-8652-260a5dd76cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl\n",
            "  Downloading dgl-1.1.1-cp310-cp310-manylinux1_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.65.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-1.1.1\n"
          ]
        }
      ],
      "source": [
        "pip install  dgl -f https://data.dgl.ai/wheels/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTEcaNXsbQFp",
        "outputId": "2001d820-3e11-4b30-beb9-eb7ea1f6cefd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels-test/repo.html\n",
            "Collecting dglgo\n",
            "  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (0.7.0)\n",
            "Collecting isort>=5.10.1 (from dglgo)\n",
            "  Downloading isort-5.12.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autopep8>=1.6.0 (from dglgo)\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpydoc>=1.1.0 (from dglgo)\n",
            "  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.10.9)\n",
            "Collecting ruamel.yaml>=0.17.20 (from dglgo)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from dglgo) (6.0)\n",
            "Collecting ogb>=1.3.3 (from dglgo)\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit-pypi (from dglgo)\n",
            "  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from dglgo) (1.2.2)\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8>=1.6.0->dglgo)\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8>=1.6.0->dglgo) (2.0.1)\n",
            "Collecting sphinx>=4.2 (from numpydoc>=1.1.0->dglgo)\n",
            "  Downloading sphinx-7.0.1-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.10/dist-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (4.65.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.3.3->dglgo) (1.26.16)\n",
            "Collecting outdated>=0.2.0 (from ogb>=1.3.3->dglgo)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.0->dglgo) (4.6.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.20->dglgo)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.4.0->dglgo) (8.1.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi->dglgo) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb>=1.3.3->dglgo)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.7.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.0.3)\n",
            "Requirement already satisfied: Pygments>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.14.0)\n",
            "Collecting docutils<0.21,>=0.18.1 (from sphinx>=4.2->numpydoc>=1.1.0->dglgo)\n",
            "  Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.7/572.7 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: snowballstemmer>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.7.13)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (1.4.1)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb>=1.3.3->dglgo) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb>=1.3.3->dglgo) (16.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb>=1.3.3->dglgo) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb>=1.3.3->dglgo) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=6f47894dbf0e9ba3ee2db7908e4c662200772a810e1a6e34287f504c37ddd51f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, ruamel.yaml.clib, rdkit-pypi, pycodestyle, isort, docutils, sphinx, ruamel.yaml, outdated, autopep8, numpydoc, ogb, dglgo\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.16\n",
            "    Uninstalling docutils-0.16:\n",
            "      Successfully uninstalled docutils-0.16\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 3.5.4\n",
            "    Uninstalling Sphinx-3.5.4:\n",
            "      Successfully uninstalled Sphinx-3.5.4\n",
            "Successfully installed autopep8-2.0.2 dglgo-0.0.2 docutils-0.20.1 isort-5.12.0 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.6 outdated-0.2.2 pycodestyle-2.10.0 rdkit-pypi-2022.9.5 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 sphinx-7.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RH7Or5WjhXu1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
        "import dgl\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import dgl.function as fn\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "from torch.utils.data import DataLoader\n",
        "import cloudpickle\n",
        "from dgl.nn import GraphConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from dgl.nn import GraphConv\n",
        "from dgl.nn import GINConv\n",
        "from dgl.nn import SAGEConv\n",
        "from dgl.nn import GATConv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z37D6dxGhXu1"
      },
      "source": [
        "#### Set Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc0AxQa33KPm",
        "outputId": "0144e453-ab8f-4c2f-ab98-488ff15773a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Oig_cfJnzMSZ"
      },
      "outputs": [],
      "source": [
        "# Define the path to the current directory where the ZIP file is located.\n",
        "current_dir = \"/content/drive/MyDrive/graph_data1.zip\"\n",
        "\n",
        "# Create the path to the directory where model checkpoints will be saved.\n",
        "checkpoint_path = current_dir + \"save_models/model_checkpoints/\" + \"checkpoint\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "# Define the path to the directory where the best model will be saved.\n",
        "best_model_path = current_dir + \"save_models/best_model/\"\n",
        "\n",
        "# Create a temporary folder path for data manipulation.\n",
        "folder_data_temp = current_dir + \"data_temp/\"\n",
        "\n",
        "# Remove the temporary folder if it exists, ignoring any errors if it does not.\n",
        "shutil.rmtree(folder_data_temp, ignore_errors=True)\n",
        "\n",
        "# Define the path to save the unpacked files from the ZIP archive.\n",
        "path_save = current_dir\n",
        "\n",
        "# Unpack the contents of the ZIP archive to the temporary folder.\n",
        "shutil.unpack_archive(path_save, folder_data_temp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4GH1uUxhXu2"
      },
      "source": [
        "#### Custom PyTorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "syHoPuso1b2e"
      },
      "outputs": [],
      "source": [
        "\"\"\" Classification Dataset \"\"\"\n",
        "\n",
        "class DGLDatasetClass(torch.utils.data.Dataset):\n",
        "    def __init__(self, address):\n",
        "        # Initialize the dataset with the given address\n",
        "        self.address = address + \".bin\"\n",
        "        self.list_graphs, train_labels_masks_globals = dgl.load_graphs(self.address)\n",
        "        num_graphs = len(self.list_graphs)\n",
        "\n",
        "        # Extract labels, masks, and globals from train_labels_masks_globals\n",
        "        self.labels = train_labels_masks_globals[\"labels\"].view(num_graphs, -1)\n",
        "        self.masks = train_labels_masks_globals[\"masks\"].view(num_graphs, -1)\n",
        "        self.globals = train_labels_masks_globals[\"globals\"].view(num_graphs, -1)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the length of the dataset\n",
        "        return len(self.list_graphs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve the item at the given index\n",
        "        return self.list_graphs[idx], self.labels[idx], self.masks[idx], self.globals[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39AeT4b4hXu2"
      },
      "source": [
        "#### Defining Train, Validation, and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aISbVq8C2bJV",
        "outputId": "d1083fa5-6f65-40c9-e728-f2ea2b3f1324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1631 203 205\n"
          ]
        }
      ],
      "source": [
        "# Define the path for temporary data\n",
        "path_data_temp = folder_data_temp + \"scaffold\" + \"_\" + str(0)\n",
        "\n",
        "# Create instances of DGLDatasetClass for the train, validation, and test sets\n",
        "train_set = DGLDatasetClass(address=path_data_temp + \"_train\")\n",
        "val_set = DGLDatasetClass(address=path_data_temp + \"_val\")\n",
        "test_set = DGLDatasetClass(address=path_data_temp + \"_test\")\n",
        "\n",
        "# Print the lengths of the train, validation, and test sets\n",
        "print(len(train_set), len(val_set), len(test_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN3c53dRhXu3"
      },
      "source": [
        "#### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lBsQwY2D5vqD"
      },
      "outputs": [],
      "source": [
        "# Define a collate function to process a batch of data samples.\n",
        "def collate(batch):\n",
        "    # Extract the graphs from the batch and create a batched graph using dgl.batch.\n",
        "    graphs = [e[0] for e in batch]\n",
        "    g = dgl.batch(graphs)\n",
        "\n",
        "    # Extract the labels from the batch and stack them into a tensor.\n",
        "    labels = [e[1] for e in batch]\n",
        "    labels = torch.stack(labels, 0)\n",
        "\n",
        "    # Extract the masks from the batch and stack them into a tensor.\n",
        "    masks = [e[2] for e in batch]\n",
        "    masks = torch.stack(masks, 0)\n",
        "\n",
        "    # Extract the global features from the batch and stack them into a tensor.\n",
        "    globals = [e[3] for e in batch]\n",
        "    globals = torch.stack(globals, 0)\n",
        "\n",
        "    # Return the batched graph, labels, masks, and globals.\n",
        "    return g, labels, masks, globals\n",
        "\n",
        "\n",
        "# Define a loader function to create data loaders for the training, validation, and test sets.\n",
        "def loader(batch_size=64):\n",
        "    # Create a data loader for the training set.\n",
        "    train_dataloader = DataLoader(train_set,\n",
        "                                  batch_size=batch_size,\n",
        "                                  collate_fn=collate,\n",
        "                                  drop_last=False,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=1)\n",
        "\n",
        "    # Create a data loader for the validation set.\n",
        "    val_dataloader = DataLoader(val_set,\n",
        "                                batch_size=batch_size,\n",
        "                                collate_fn=collate,\n",
        "                                drop_last=False,\n",
        "                                shuffle=False,\n",
        "                                num_workers=1)\n",
        "\n",
        "    # Create a data loader for the test set.\n",
        "    test_dataloader = DataLoader(test_set,\n",
        "                                 batch_size=batch_size,\n",
        "                                 collate_fn=collate,\n",
        "                                 drop_last=False,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=1) #The number of worker threads to use for loading the data.\n",
        "\n",
        "    # Return the data loaders for training, validation, and test sets.\n",
        "    return train_dataloader, val_dataloader, test_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zHCkULH5hXu3"
      },
      "outputs": [],
      "source": [
        "# Create data loaders for the training, validation, and test sets with a batch size of 64.\n",
        "train_dataloader, val_dataloader, test_dataloader = loader(batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBRkhkI3hXu3"
      },
      "source": [
        "#### Defining A GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDru1hnyhXu3"
      },
      "source": [
        "##### Some Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CFPaFdHkhXu3"
      },
      "outputs": [],
      "source": [
        "#Bace dataset has 1 task. Some other datasets may have some more number of tasks, e.g., tox21 has 12 tasks.\n",
        "num_tasks = 1\n",
        "\n",
        "# Size of global feature of each graph\n",
        "global_size = 200\n",
        "\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 100\n",
        "\n",
        "# Number of steps to wait if the model performance on the validation set does not improve\n",
        "patience = 10\n",
        "\n",
        "#Configurations to instantiate the model\n",
        "config = {\"node_feature_size\":127, \"edge_feature_size\":12, \"hidden_size\":100}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppT6foNEvkSB"
      },
      "source": [
        "#GCN 2Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QoPRoiI5hXu3"
      },
      "outputs": [],
      "source": [
        "#Define a GNN (Graph Neural Network) class as a subclass of nn.Module(This GNN model can be used for graph classification tasks on molecular graphs. \n",
        "# The forward pass takes a DGL graph object and the global features as input and computes the node representations for each task)\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        # Create the first GraphConv layer with input size equal to the node feature size and output size equal to the hidden size.\n",
        "        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size, allow_zero_in_degree = True)\n",
        "\n",
        "        # Create the second GraphConv layer with input size equal to the hidden size and output size equal to the number of tasks\n",
        "        self.conv2 = GraphConv(self.hidden_size, self.num_tasks, allow_zero_in_degree = True)\n",
        "\n",
        "    # def forward(self, g, in_feat):\n",
        "def forward(self, mol_dgl_graph, globals):\n",
        "    # Reduce the node feature dimensionality\n",
        "    mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n",
        "    # Reduce the edge feature dimensionality\n",
        "    mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n",
        "    # Perform convolution operation on the graph with the node features\n",
        "    h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "    # Apply ReLU activation function\n",
        "    h = F.relu(h)\n",
        "    # Perform another convolution operation on the graph with the updated features\n",
        "    h = self.conv2(mol_dgl_graph, h)\n",
        "    # Assign the updated features to the node data\n",
        "    mol_dgl_graph.ndata[\"h\"] = h\n",
        "    # Return the mean of the node features across the graph\n",
        "    return dgl.mean_nodes(mol_dgl_graph, \"h\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzqdwrdohXu4"
      },
      "source": [
        "#### Function to Compute Score of the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "U1pu-l6hSvvl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Define the metric as roc_auc_score\n",
        "    metric = roc_auc_score\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Initialize empty tensors for storing predictions, labels, and masks\n",
        "        prediction_all = torch.empty(0)\n",
        "        labels_all = torch.empty(0)\n",
        "        masks_all = torch.empty(0)\n",
        "\n",
        "        # Iterate over the data_loader\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            # Compute predictions from the model\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "\n",
        "            # Concatenate predictions, labels, and masks to the respective tensors\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "\n",
        "        # Initialize average score tensor\n",
        "        average = torch.tensor([0.])\n",
        "\n",
        "        # Compute the metric for each task\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:, i] == 1]\n",
        "            a2 = labels_all[:, i][masks_all[:, i] == 1]\n",
        "\n",
        "            try:\n",
        "                # Calculate the metric score\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                # Handle the case where the metric calculation throws a ValueError\n",
        "                t = 0\n",
        "\n",
        "            average += t\n",
        "\n",
        "    # Return the average score divided by the number of tasks\n",
        "    return average.item() / num_tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNOajUO7hXu4"
      },
      "source": [
        "#### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RIa0EBikhXu4"
      },
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "def loss_func(output, label, mask, num_tasks):\n",
        "    # Create a tensor of ones as positive weights for BCEWithLogitsLoss\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "\n",
        "    # Define the criterion as BCEWithLogitsLoss with no reduction and positive weights\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "\n",
        "    # Compute the element-wise loss by applying the mask to the criterion output\n",
        "    loss = mask * criterion(output, label)\n",
        "\n",
        "    # Compute the average loss by summing the masked loss values and dividing by the sum of the mask values\n",
        "    loss = loss.sum() / mask.sum()\n",
        "\n",
        "    # Return the computed loss\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8aquB5ehXu4"
      },
      "source": [
        "#### Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fgQkoc7hXu4"
      },
      "source": [
        "##### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZmR38IFxhXu4"
      },
      "outputs": [],
      "source": [
        "# Define a function to train a single epoch using the given training data loader, model, and optimizer.\n",
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    # Initialize the epoch train loss and iterations.\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "\n",
        "    # Set the model to train mode.\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over the training data loader.\n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        # Make predictions using the model.\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "\n",
        "        # Compute the training loss using the loss function.\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "\n",
        "        # Zero the gradients of the model parameters.\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Perform backpropagation to compute gradients.\n",
        "        loss_train.backward()\n",
        "\n",
        "        # Update the model parameters using the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the training loss.\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "\n",
        "        # Increment the iterations count.\n",
        "        iterations += 1\n",
        "\n",
        "    # Compute the average epoch train loss.\n",
        "    epoch_train_loss /= iterations\n",
        "\n",
        "    # Return the average epoch train loss.\n",
        "    return epoch_train_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mx02YIrihXu5"
      },
      "outputs": [],
      "source": [
        "# Define a function to train and evaluate the model.\n",
        "def train_evaluate():\n",
        "    # Create a new instance of the GNN model with the given configuration.\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "\n",
        "    # Create an Adam optimizer for training the model.\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    # Initialize variables for tracking the best validation score and patience count.\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    # Continue training until reaching the maximum number of epochs.\n",
        "    while epoch <= num_epochs:\n",
        "        # Check if the patience count is within the allowed limit.\n",
        "        if patience_count <= patience:\n",
        "            # Set the model to train mode and compute the training loss for the current epoch.\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "\n",
        "            # Set the model to eval mode and compute the validation score.\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "\n",
        "            # Check if the current validation score is better than the best validation score so far.\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "\n",
        "                # Create a dictionary to store the checkpoint information.\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "\n",
        "                # Save the checkpoint to a file using cloudpickle.\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            # Print the training and validation scores for the current epoch.\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(epoch, num_epochs, loss_train, score_val))\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    # Save the best model by copying the checkpoint directory.\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    # Print the final results.\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twE7VJvQhXu5"
      },
      "source": [
        "##### Function to compute test set score of the final saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OQblpT0hXu5"
      },
      "source": [
        "##### Train the model and evaluate its performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2Ivm_9-PIRxE"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    # Create the final model\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "\n",
        "    # Set the path to the best model checkpoint file\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "\n",
        "    # Open the best model checkpoint file and load it\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "\n",
        "    # Load the state dictionary of the best model\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Set the final model to evaluation mode\n",
        "    final_model.eval()\n",
        "\n",
        "    # Compute the test score\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    # Print the test score\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "\n",
        "    # Print the execution time\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11d-SdbahXu5",
        "outputId": "deda97b1-f8ee-44db-b304-622249da879a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.641 | Valid Score: 0.254\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.254 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.620 | Valid Score: 0.264\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.264 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 0.608 | Valid Score: 0.271\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.271 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 4/100 | Training Loss: 0.601 | Valid Score: 0.281\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.281 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 0.595 | Valid Score: 0.290\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.290 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 6/100 | Training Loss: 0.590 | Valid Score: 0.303\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.303 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.588 | Valid Score: 0.319\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.319 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.584 | Valid Score: 0.336\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.336 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.581 | Valid Score: 0.353\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.353 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.577 | Valid Score: 0.369\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.369 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 11/100 | Training Loss: 0.572 | Valid Score: 0.391\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.391 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 0.569 | Valid Score: 0.407\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.407 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 0.567 | Valid Score: 0.430\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.430 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 0.568 | Valid Score: 0.456\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.456 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 15/100 | Training Loss: 0.563 | Valid Score: 0.495\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.495 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 0.559 | Valid Score: 0.533\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.533 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 0.552 | Valid Score: 0.559\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.559 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 18/100 | Training Loss: 0.549 | Valid Score: 0.582\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.582 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 19/100 | Training Loss: 0.548 | Valid Score: 0.600\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.600 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 20/100 | Training Loss: 0.548 | Valid Score: 0.617\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.617 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 21/100 | Training Loss: 0.540 | Valid Score: 0.655\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.655 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 22/100 | Training Loss: 0.538 | Valid Score: 0.667\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.667 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 23/100 | Training Loss: 0.536 | Valid Score: 0.683\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.683 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 24/100 | Training Loss: 0.530 | Valid Score: 0.703\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 0.703 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 25/100 | Training Loss: 0.529 | Valid Score: 0.711\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 0.711 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 26/100 | Training Loss: 0.526 | Valid Score: 0.720\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 0.720 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 27/100 | Training Loss: 0.523 | Valid Score: 0.735\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 0.735 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 28/100 | Training Loss: 0.521 | Valid Score: 0.746\n",
            " \n",
            "Epoch: 28/100 | Best Valid Score Until Now: 0.746 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 29/100 | Training Loss: 0.518 | Valid Score: 0.750\n",
            " \n",
            "Epoch: 29/100 | Best Valid Score Until Now: 0.750 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 30/100 | Training Loss: 0.514 | Valid Score: 0.759\n",
            " \n",
            "Epoch: 30/100 | Best Valid Score Until Now: 0.759 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 31/100 | Training Loss: 0.512 | Valid Score: 0.765\n",
            " \n",
            "Epoch: 31/100 | Best Valid Score Until Now: 0.765 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 32/100 | Training Loss: 0.511 | Valid Score: 0.769\n",
            " \n",
            "Epoch: 32/100 | Best Valid Score Until Now: 0.769 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 33/100 | Training Loss: 0.505 | Valid Score: 0.775\n",
            " \n",
            "Epoch: 33/100 | Best Valid Score Until Now: 0.775 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 34/100 | Training Loss: 0.505 | Valid Score: 0.778\n",
            " \n",
            "Epoch: 34/100 | Best Valid Score Until Now: 0.778 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 35/100 | Training Loss: 0.502 | Valid Score: 0.781\n",
            " \n",
            "Epoch: 35/100 | Best Valid Score Until Now: 0.781 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 36/100 | Training Loss: 0.498 | Valid Score: 0.787\n",
            " \n",
            "Epoch: 36/100 | Best Valid Score Until Now: 0.787 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 37/100 | Training Loss: 0.495 | Valid Score: 0.787\n",
            " \n",
            "Epoch: 37/100 | Best Valid Score Until Now: 0.787 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 38/100 | Training Loss: 0.495 | Valid Score: 0.793\n",
            " \n",
            "Epoch: 38/100 | Best Valid Score Until Now: 0.793 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 39/100 | Training Loss: 0.495 | Valid Score: 0.794\n",
            " \n",
            "Epoch: 39/100 | Best Valid Score Until Now: 0.794 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 40/100 | Training Loss: 0.491 | Valid Score: 0.797\n",
            " \n",
            "Epoch: 40/100 | Best Valid Score Until Now: 0.797 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 41/100 | Training Loss: 0.486 | Valid Score: 0.797\n",
            " \n",
            "Epoch: 41/100 | Best Valid Score Until Now: 0.797 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 42/100 | Training Loss: 0.487 | Valid Score: 0.800\n",
            " \n",
            "Epoch: 42/100 | Best Valid Score Until Now: 0.800 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 43/100 | Training Loss: 0.483 | Valid Score: 0.801\n",
            " \n",
            "Epoch: 43/100 | Best Valid Score Until Now: 0.801 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 44/100 | Training Loss: 0.483 | Valid Score: 0.803\n",
            " \n",
            "Epoch: 44/100 | Best Valid Score Until Now: 0.803 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 45/100 | Training Loss: 0.478 | Valid Score: 0.806\n",
            " \n",
            "Epoch: 45/100 | Best Valid Score Until Now: 0.806 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 46/100 | Training Loss: 0.479 | Valid Score: 0.807\n",
            " \n",
            "Epoch: 46/100 | Best Valid Score Until Now: 0.807 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 47/100 | Training Loss: 0.474 | Valid Score: 0.808\n",
            " \n",
            "Epoch: 47/100 | Best Valid Score Until Now: 0.808 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 48/100 | Training Loss: 0.471 | Valid Score: 0.808\n",
            " \n",
            "Epoch: 48/100 | Best Valid Score Until Now: 0.808 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 49/100 | Training Loss: 0.473 | Valid Score: 0.810\n",
            " \n",
            "Epoch: 49/100 | Best Valid Score Until Now: 0.810 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 50/100 | Training Loss: 0.472 | Valid Score: 0.810\n",
            " \n",
            "Epoch: 50/100 | Best Valid Score Until Now: 0.810 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 51/100 | Training Loss: 0.467 | Valid Score: 0.812\n",
            " \n",
            "Epoch: 51/100 | Best Valid Score Until Now: 0.812 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 52/100 | Training Loss: 0.469 | Valid Score: 0.813\n",
            " \n",
            "Epoch: 52/100 | Best Valid Score Until Now: 0.813 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 53/100 | Training Loss: 0.466 | Valid Score: 0.813\n",
            " \n",
            "Epoch: 53/100 | Best Valid Score Until Now: 0.813 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 54/100 | Training Loss: 0.463 | Valid Score: 0.815\n",
            " \n",
            "Epoch: 54/100 | Best Valid Score Until Now: 0.815 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 55/100 | Training Loss: 0.461 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 55/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 56/100 | Training Loss: 0.459 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 56/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 57/100 | Training Loss: 0.462 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 57/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 58/100 | Training Loss: 0.461 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 58/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 59/100 | Training Loss: 0.457 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 59/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 60/100 | Training Loss: 0.454 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 60/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 61/100 | Training Loss: 0.456 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 61/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 62/100 | Training Loss: 0.454 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 62/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 63/100 | Training Loss: 0.455 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 63/100 | Best Valid Score Until Now: 0.818 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 64/100 | Training Loss: 0.451 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 64/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 65/100 | Training Loss: 0.450 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 65/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 66/100 | Training Loss: 0.452 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 66/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 67/100 | Training Loss: 0.452 | Valid Score: 0.818\n",
            " \n",
            "Epoch: 67/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 68/100 | Training Loss: 0.449 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 68/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 69/100 | Training Loss: 0.446 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 69/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 70/100 | Training Loss: 0.448 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 70/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 71/100 | Training Loss: 0.442 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 71/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 72/100 | Training Loss: 0.448 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 72/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 73/100 | Training Loss: 0.446 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 73/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 74/100 | Training Loss: 0.443 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 74/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 75/100 | Training Loss: 0.440 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 75/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 76/100 | Training Loss: 0.442 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 76/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 77/100 | Training Loss: 0.439 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 77/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 78/100 | Training Loss: 0.439 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 78/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 79/100 | Training Loss: 0.439 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 79/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 80/100 | Training Loss: 0.439 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 80/100 | Best Valid Score Until Now: 0.820 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 81/100 | Training Loss: 0.439 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 81/100 | Best Valid Score Until Now: 0.821 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 82/100 | Training Loss: 0.439 | Valid Score: 0.820\n",
            " \n",
            "Epoch: 82/100 | Best Valid Score Until Now: 0.821 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 83/100 | Training Loss: 0.437 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 83/100 | Best Valid Score Until Now: 0.821 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 84/100 | Training Loss: 0.433 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 84/100 | Best Valid Score Until Now: 0.821 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 85/100 | Training Loss: 0.433 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 85/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 86/100 | Training Loss: 0.433 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 86/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 87/100 | Training Loss: 0.437 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 87/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 88/100 | Training Loss: 0.434 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 88/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 89/100 | Training Loss: 0.434 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 89/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 90/100 | Training Loss: 0.431 | Valid Score: 0.823\n",
            " \n",
            "Epoch: 90/100 | Best Valid Score Until Now: 0.823 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 91/100 | Training Loss: 0.435 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 91/100 | Best Valid Score Until Now: 0.823 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 92/100 | Training Loss: 0.432 | Valid Score: 0.823\n",
            " \n",
            "Epoch: 92/100 | Best Valid Score Until Now: 0.823 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 93/100 | Training Loss: 0.432 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 93/100 | Best Valid Score Until Now: 0.823 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 94/100 | Training Loss: 0.432 | Valid Score: 0.823\n",
            " \n",
            "Epoch: 94/100 | Best Valid Score Until Now: 0.823 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 95/100 | Training Loss: 0.434 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 95/100 | Best Valid Score Until Now: 0.823 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 96/100 | Training Loss: 0.430 | Valid Score: 0.824\n",
            " \n",
            "Epoch: 96/100 | Best Valid Score Until Now: 0.824 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 97/100 | Training Loss: 0.429 | Valid Score: 0.824\n",
            " \n",
            "Epoch: 97/100 | Best Valid Score Until Now: 0.824 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 98/100 | Training Loss: 0.428 | Valid Score: 0.823\n",
            " \n",
            "Epoch: 98/100 | Best Valid Score Until Now: 0.824 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 99/100 | Training Loss: 0.431 | Valid Score: 0.823\n",
            " \n",
            "Epoch: 99/100 | Best Valid Score Until Now: 0.824 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 100/100 | Training Loss: 0.427 | Valid Score: 0.824\n",
            " \n",
            "Epoch: 100/100 | Best Valid Score Until Now: 0.824 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.824 \n",
            "\n",
            "Test Score: 0.634 \n",
            "\n",
            "Execution time: 56.835 seconds\n"
          ]
        }
      ],
      "source": [
        "#This line imports the time module, which provides various time-related functions\n",
        "import time\n",
        "#This line records the current time using time.time() and assigns it to the variable start_time. It serves as the starting point for measuring the execution time\n",
        "start_time = time.time()\n",
        "#This line calls the train_evaluate() function, which is likely responsible for training and evaluating a model.\n",
        "train_evaluate()\n",
        "#This line calls the test_evaluate() function, which probably performs evaluation on a separate test dataset.\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB8RtVlNzP8g"
      },
      "source": [
        "#GCN 3Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ALTwOSYWzUt-"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        \n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        \n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "       \n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = GraphConv(self.node_feature_size, self.hidden_size,allow_zero_in_degree=True)\n",
        "        self.conv2 = GraphConv(self.hidden_size, self.hidden_size,allow_zero_in_degree=True)\n",
        "        self.conv3 = GraphConv(self.hidden_size, self.num_tasks,allow_zero_in_degree=True)\n",
        "\n",
        "    \n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv3(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "DlK6HGZyzVFK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    model.eval()\n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                t = 0\n",
        "            average += t\n",
        "    return average.item()/num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "adY6RMQgzVIZ"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "   \n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jRq1IvgkzVLc"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() \n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "DoR3oUEuzVOr"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    \n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OdVVVil_zVSK"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lxfhOklzVVs",
        "outputId": "fd854aa0-0b14-4bde-d064-57bca878d29c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.666 | Valid Score: 0.237\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.237 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.629 | Valid Score: 0.248\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.248 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 0.611 | Valid Score: 0.259\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.259 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 4/100 | Training Loss: 0.601 | Valid Score: 0.275\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.275 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 0.593 | Valid Score: 0.298\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.298 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 6/100 | Training Loss: 0.584 | Valid Score: 0.328\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.328 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.577 | Valid Score: 0.373\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.373 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.570 | Valid Score: 0.451\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.451 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.559 | Valid Score: 0.528\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.528 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.558 | Valid Score: 0.595\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.595 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 11/100 | Training Loss: 0.548 | Valid Score: 0.651\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.651 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 0.539 | Valid Score: 0.699\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.699 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 0.535 | Valid Score: 0.747\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.747 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 0.529 | Valid Score: 0.795\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.795 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 15/100 | Training Loss: 0.521 | Valid Score: 0.809\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.809 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 0.514 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 0.504 | Valid Score: 0.828\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.828 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 18/100 | Training Loss: 0.498 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 19/100 | Training Loss: 0.492 | Valid Score: 0.839\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 20/100 | Training Loss: 0.483 | Valid Score: 0.841\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.841 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 21/100 | Training Loss: 0.481 | Valid Score: 0.843\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.843 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 22/100 | Training Loss: 0.476 | Valid Score: 0.846\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.846 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 23/100 | Training Loss: 0.471 | Valid Score: 0.846\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.846 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 24/100 | Training Loss: 0.463 | Valid Score: 0.847\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 0.847 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 25/100 | Training Loss: 0.459 | Valid Score: 0.848\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 26/100 | Training Loss: 0.457 | Valid Score: 0.847\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 27/100 | Training Loss: 0.454 | Valid Score: 0.846\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 28/100 | Training Loss: 0.448 | Valid Score: 0.845\n",
            " \n",
            "Epoch: 28/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 29/100 | Training Loss: 0.446 | Valid Score: 0.846\n",
            " \n",
            "Epoch: 29/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 30/100 | Training Loss: 0.444 | Valid Score: 0.846\n",
            " \n",
            "Epoch: 30/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 31/100 | Training Loss: 0.443 | Valid Score: 0.845\n",
            " \n",
            "Epoch: 31/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 32/100 | Training Loss: 0.439 | Valid Score: 0.846\n",
            " \n",
            "Epoch: 32/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 33/100 | Training Loss: 0.437 | Valid Score: 0.846\n",
            " \n",
            "Epoch: 33/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 34/100 | Training Loss: 0.437 | Valid Score: 0.847\n",
            " \n",
            "Epoch: 34/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 35/100 | Training Loss: 0.431 | Valid Score: 0.847\n",
            " \n",
            "Epoch: 35/100 | Best Valid Score Until Now: 0.848 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.848 \n",
            "\n",
            "Test Score: 0.585 \n",
            "\n",
            "Execution time: 23.226 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q1e8nb-TafH"
      },
      "source": [
        "#GraphSAGE 2Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "v6Ts7zalTjlf"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        \n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        \n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        \n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size, aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(self.hidden_size, self.num_tasks, aggregator_type='mean')\n",
        "\n",
        "    \n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rx1Bd80yUTYq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    \n",
        "    model.eval()\n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        \n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                t = 0\n",
        "            average += t\n",
        "    return average.item()/num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vVUvNEhBUoBQ"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZvyI7Tm_UxhE"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() \n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NCDBMbReUyeb"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "    while epoch <= num_epochs:\n",
        "       \n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            \n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                \n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    \n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "    \n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jJ7gNApeU1Vw"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnFZzmDMU5vz",
        "outputId": "3c4dafad-c8af-4dc5-cef3-de45ffcb1251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.538 | Valid Score: 0.795\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.795 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.473 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.821 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 0.438 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 4/100 | Training Loss: 0.422 | Valid Score: 0.827\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 5/100 | Training Loss: 0.421 | Valid Score: 0.824\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 6/100 | Training Loss: 0.405 | Valid Score: 0.828\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 7/100 | Training Loss: 0.399 | Valid Score: 0.827\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.396 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 9/100 | Training Loss: 0.389 | Valid Score: 0.826\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.387 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 11/100 | Training Loss: 0.380 | Valid Score: 0.828\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 0.379 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.830 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 0.376 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 14/100 | Training Loss: 0.382 | Valid Score: 0.831\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 15/100 | Training Loss: 0.379 | Valid Score: 0.828\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 16/100 | Training Loss: 0.377 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 17/100 | Training Loss: 0.364 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 18/100 | Training Loss: 0.364 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 19/100 | Training Loss: 0.362 | Valid Score: 0.823\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 20/100 | Training Loss: 0.361 | Valid Score: 0.831\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 21/100 | Training Loss: 0.359 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 22/100 | Training Loss: 0.357 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 23/100 | Training Loss: 0.350 | Valid Score: 0.810\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.834 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.834 \n",
            "\n",
            "Test Score: 0.727 \n",
            "\n",
            "Execution time: 13.317 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufp1r0x_0Qwu"
      },
      "source": [
        "#GraphSAGE 3Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Zlu6e3002T4T"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size=200, num_tasks=1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = SAGEConv(self.node_feature_size, self.hidden_size,aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(self.hidden_size, self.hidden_size,aggregator_type='mean')\n",
        "        self.conv3 = SAGEConv(self.hidden_size, self.num_tasks,aggregator_type='mean')\n",
        "\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n",
        "\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv3(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "AGcH_4OC2UGf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    model.eval()\n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                t = 0\n",
        "            average += t\n",
        "    return average.item()/num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3ZlpB3FJ2UJd"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "y2yfewc22UMe"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() \n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        loss_train.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "DDNtZMIO2UPO"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    \n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "hWnd5itj2aum"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j5msG4c4dcv",
        "outputId": "d703c30f-c109-4a67-a1ba-19f972daa9a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.642 | Valid Score: 0.413\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.413 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.587 | Valid Score: 0.579\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.579 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 0.545 | Valid Score: 0.731\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.731 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 4/100 | Training Loss: 0.518 | Valid Score: 0.809\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.809 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 0.500 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.829 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 6/100 | Training Loss: 0.490 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.830 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.475 | Valid Score: 0.837\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.837 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.462 | Valid Score: 0.839\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.452 | Valid Score: 0.839\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.442 | Valid Score: 0.842\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 11/100 | Training Loss: 0.435 | Valid Score: 0.840\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 12/100 | Training Loss: 0.428 | Valid Score: 0.840\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 13/100 | Training Loss: 0.421 | Valid Score: 0.841\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 14/100 | Training Loss: 0.415 | Valid Score: 0.841\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 15/100 | Training Loss: 0.413 | Valid Score: 0.841\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 16/100 | Training Loss: 0.407 | Valid Score: 0.839\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 17/100 | Training Loss: 0.402 | Valid Score: 0.840\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 18/100 | Training Loss: 0.400 | Valid Score: 0.840\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 19/100 | Training Loss: 0.401 | Valid Score: 0.841\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 20/100 | Training Loss: 0.401 | Valid Score: 0.840\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.842 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.842 \n",
            "\n",
            "Test Score: 0.616 \n",
            "\n",
            "Execution time: 12.964 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxg6IKGrXrPg"
      },
      "source": [
        "#GIN 2Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "1EZQ96RCXp7_"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = GINConv(nn.Linear(self.node_feature_size, self.hidden_size), aggregator_type='sum')\n",
        "        self.conv2 = GINConv(nn.Linear(self.hidden_size, self.num_tasks), aggregator_type='sum')\n",
        "\n",
        "    \n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"]= mol_dgl_graph.ndata[\"v\"][:,:self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:,:self.edge_feature_size]\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "zLHFTlNeXoE0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "  \n",
        "    model.eval()\n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "       \n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        \n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "           \n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                t = 0\n",
        "            average += t\n",
        "    return average.item()/num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "zUHAgLHFYj_E"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "n1mKNaxnYtBg"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() \n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "R4SeyqXlYv2Q"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    \n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "mpU4eeTYYzKa"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mRmQSbrY184",
        "outputId": "ce415d53-9986-4f11-816c-3932613661ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.655 | Valid Score: 0.242\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.242 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.622 | Valid Score: 0.273\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.273 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 0.609 | Valid Score: 0.316\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.316 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 4/100 | Training Loss: 0.599 | Valid Score: 0.378\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.378 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 0.588 | Valid Score: 0.444\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.444 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 6/100 | Training Loss: 0.577 | Valid Score: 0.495\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.495 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.565 | Valid Score: 0.559\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.559 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.557 | Valid Score: 0.617\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.617 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.543 | Valid Score: 0.676\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.676 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.533 | Valid Score: 0.701\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.701 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 11/100 | Training Loss: 0.521 | Valid Score: 0.738\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.738 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 12/100 | Training Loss: 0.513 | Valid Score: 0.758\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.758 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 13/100 | Training Loss: 0.506 | Valid Score: 0.767\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.767 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 14/100 | Training Loss: 0.494 | Valid Score: 0.791\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.791 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 15/100 | Training Loss: 0.489 | Valid Score: 0.791\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.791 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 16/100 | Training Loss: 0.487 | Valid Score: 0.797\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.797 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 0.478 | Valid Score: 0.806\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.806 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 18/100 | Training Loss: 0.473 | Valid Score: 0.808\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.808 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 19/100 | Training Loss: 0.467 | Valid Score: 0.813\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.813 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 20/100 | Training Loss: 0.467 | Valid Score: 0.813\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.813 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 21/100 | Training Loss: 0.461 | Valid Score: 0.816\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 22/100 | Training Loss: 0.460 | Valid Score: 0.815\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.816 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 23/100 | Training Loss: 0.459 | Valid Score: 0.822\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.822 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 24/100 | Training Loss: 0.453 | Valid Score: 0.824\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 0.824 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 25/100 | Training Loss: 0.449 | Valid Score: 0.825\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 0.825 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 26/100 | Training Loss: 0.443 | Valid Score: 0.823\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 0.825 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 27/100 | Training Loss: 0.444 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 0.830 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 28/100 | Training Loss: 0.446 | Valid Score: 0.827\n",
            " \n",
            "Epoch: 28/100 | Best Valid Score Until Now: 0.830 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 29/100 | Training Loss: 0.441 | Valid Score: 0.828\n",
            " \n",
            "Epoch: 29/100 | Best Valid Score Until Now: 0.830 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 30/100 | Training Loss: 0.440 | Valid Score: 0.831\n",
            " \n",
            "Epoch: 30/100 | Best Valid Score Until Now: 0.831 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 31/100 | Training Loss: 0.439 | Valid Score: 0.832\n",
            " \n",
            "Epoch: 31/100 | Best Valid Score Until Now: 0.832 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 32/100 | Training Loss: 0.434 | Valid Score: 0.828\n",
            " \n",
            "Epoch: 32/100 | Best Valid Score Until Now: 0.832 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 33/100 | Training Loss: 0.439 | Valid Score: 0.836\n",
            " \n",
            "Epoch: 33/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 34/100 | Training Loss: 0.432 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 34/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 35/100 | Training Loss: 0.436 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 35/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 36/100 | Training Loss: 0.434 | Valid Score: 0.832\n",
            " \n",
            "Epoch: 36/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 37/100 | Training Loss: 0.433 | Valid Score: 0.833\n",
            " \n",
            "Epoch: 37/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 38/100 | Training Loss: 0.431 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 38/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 39/100 | Training Loss: 0.430 | Valid Score: 0.833\n",
            " \n",
            "Epoch: 39/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 40/100 | Training Loss: 0.431 | Valid Score: 0.832\n",
            " \n",
            "Epoch: 40/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 41/100 | Training Loss: 0.431 | Valid Score: 0.832\n",
            " \n",
            "Epoch: 41/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 42/100 | Training Loss: 0.428 | Valid Score: 0.832\n",
            " \n",
            "Epoch: 42/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 43/100 | Training Loss: 0.429 | Valid Score: 0.831\n",
            " \n",
            "Epoch: 43/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.836 \n",
            "\n",
            "Test Score: 0.612 \n",
            "\n",
            "Execution time: 25.877 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSvrARgI6DWO"
      },
      "source": [
        "#GIN 3Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Myqa6J-p6LOp"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size=200, num_tasks=1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.conv1 = GINConv(nn.Linear(self.node_feature_size, self.hidden_size), aggregator_type='sum')\n",
        "        self.conv2 = GINConv(nn.Linear(self.hidden_size, self.hidden_size), aggregator_type='sum')\n",
        "        self.conv3 = GINConv(nn.Linear(self.hidden_size, self.num_tasks), aggregator_type='sum')\n",
        "\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n",
        "\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"])\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(mol_dgl_graph, h)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv3(mol_dgl_graph, h)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "6WJ4ahnp6LWa"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_score(model, data_loader, val_size, num_tasks):\n",
        "    model.eval()\n",
        "    metric = roc_auc_score\n",
        "    with torch.no_grad():\n",
        "        prediction_all= torch.empty(0)\n",
        "        labels_all= torch.empty(0)\n",
        "        masks_all= torch.empty(0)\n",
        "        for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "            prediction = model(mol_dgl_graph, globals)\n",
        "            prediction = torch.sigmoid(prediction)\n",
        "            prediction_all = torch.cat((prediction_all, prediction), 0)\n",
        "            labels_all = torch.cat((labels_all, labels), 0)\n",
        "            masks_all = torch.cat((masks_all, masks), 0)\n",
        "        average = torch.tensor([0.])\n",
        "        for i in range(num_tasks):\n",
        "            a1 = prediction_all[:, i][masks_all[:,i]==1]\n",
        "            a2 = labels_all[:, i][masks_all[:,i]==1]\n",
        "            try:\n",
        "                t = metric(a2.int().cpu(), a1.cpu()).item()\n",
        "            except ValueError:\n",
        "                t = 0\n",
        "            average += t\n",
        "    return average.item()/num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "qECGwtB16Ldg"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "2NOCB40H6Ljw"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() \n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "zj_0ICtG6Lq5"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "EqsnSFPc6Lxm"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFp_3NLW6L4W",
        "outputId": "85ce9c93-d8e8-405f-9795-d3d52fe2a12b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.607 | Valid Score: 0.483\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.483 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 2/100 | Training Loss: 0.573 | Valid Score: 0.574\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.574 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 3/100 | Training Loss: 0.548 | Valid Score: 0.747\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.747 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 4/100 | Training Loss: 0.526 | Valid Score: 0.738\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.747 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 5/100 | Training Loss: 0.506 | Valid Score: 0.784\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.784 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 6/100 | Training Loss: 0.492 | Valid Score: 0.772\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.784 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 7/100 | Training Loss: 0.481 | Valid Score: 0.813\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.813 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 8/100 | Training Loss: 0.470 | Valid Score: 0.819\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.819 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 9/100 | Training Loss: 0.464 | Valid Score: 0.825\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.825 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 10/100 | Training Loss: 0.455 | Valid Score: 0.836\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 11/100 | Training Loss: 0.450 | Valid Score: 0.828\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 12/100 | Training Loss: 0.441 | Valid Score: 0.836\n",
            " \n",
            "Epoch: 12/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 13/100 | Training Loss: 0.439 | Valid Score: 0.830\n",
            " \n",
            "Epoch: 13/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 14/100 | Training Loss: 0.432 | Valid Score: 0.833\n",
            " \n",
            "Epoch: 14/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 15/100 | Training Loss: 0.435 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 15/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 16/100 | Training Loss: 0.432 | Valid Score: 0.821\n",
            " \n",
            "Epoch: 16/100 | Best Valid Score Until Now: 0.836 \n",
            "\n",
            "Save checkpoint\n",
            "Epoch: 17/100 | Training Loss: 0.426 | Valid Score: 0.839\n",
            " \n",
            "Epoch: 17/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 18/100 | Training Loss: 0.427 | Valid Score: 0.829\n",
            " \n",
            "Epoch: 18/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 19/100 | Training Loss: 0.416 | Valid Score: 0.838\n",
            " \n",
            "Epoch: 19/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 20/100 | Training Loss: 0.417 | Valid Score: 0.839\n",
            " \n",
            "Epoch: 20/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 21/100 | Training Loss: 0.416 | Valid Score: 0.831\n",
            " \n",
            "Epoch: 21/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 22/100 | Training Loss: 0.416 | Valid Score: 0.835\n",
            " \n",
            "Epoch: 22/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 23/100 | Training Loss: 0.409 | Valid Score: 0.831\n",
            " \n",
            "Epoch: 23/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 24/100 | Training Loss: 0.408 | Valid Score: 0.836\n",
            " \n",
            "Epoch: 24/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 25/100 | Training Loss: 0.414 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 25/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 26/100 | Training Loss: 0.411 | Valid Score: 0.824\n",
            " \n",
            "Epoch: 26/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 27/100 | Training Loss: 0.408 | Valid Score: 0.834\n",
            " \n",
            "Epoch: 27/100 | Best Valid Score Until Now: 0.839 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.839 \n",
            "\n",
            "Test Score: 0.605 \n",
            "\n",
            "Execution time: 22.087 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8guhNEzWaUjs"
      },
      "source": [
        "#GAT 2Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "iPCF-q_-aZdk"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size = 200, num_tasks = 1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "        self.num_heads = self.config.get('num_heads', 1)\n",
        "        self.dropout = self.config.get('dropout', 0.0)\n",
        "\n",
        "        self.conv1 = GATConv(\n",
        "            self.node_feature_size,\n",
        "            self.hidden_size,\n",
        "            num_heads=self.num_heads,\n",
        "            feat_drop=self.dropout,\n",
        "            attn_drop=self.dropout,allow_zero_in_degree=True)\n",
        "\n",
        "        self.fc = nn.Linear(\n",
        "            self.hidden_size * self.num_heads,\n",
        "            self.hidden_size)\n",
        "        self.conv2 = GATConv(\n",
        "            self.hidden_size,\n",
        "            self.num_tasks,\n",
        "            num_heads=1,\n",
        "            feat_drop=self.dropout,\n",
        "            attn_drop=self.dropout,allow_zero_in_degree=True\n",
        "       )\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n",
        "\n",
        "        # First GAT layer\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"]).flatten(1)\n",
        "        h = F.relu(h)\n",
        "        h = self.fc(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Second GAT layer\n",
        "        h = self.conv2(mol_dgl_graph, h).squeeze(1)\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "\n",
        "        return dgl.mean_nodes(mol_dgl_graph, \"h\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "l9VHN-Foatki"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def compute_score(model, data_loader, val_size, num_tasks, scaler=None):\n",
        "  model.eval()\n",
        "  loss_sum = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n",
        "  final_loss = 0\n",
        "  state = torch.get_rng_state()\n",
        "  with torch.no_grad():\n",
        "            for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "                prediction = model(mol_dgl_graph, globals)\n",
        "                if scaler is not None:\n",
        "                  prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "                  labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "                loss = loss_sum(prediction, labels)\n",
        "                final_loss += loss.item()\n",
        "            final_loss /= val_size\n",
        "            final_loss = math.sqrt(final_loss) # RMSE\n",
        "  return final_loss / num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "KZWh1gB4axYp"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "sydHMCUFaxhk"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() \n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "mxQy9Sl_axlE"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    \n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "iE7CR7obaxn4"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Odtf9Rva-Zo",
        "outputId": "525d0f66-de15-4624-bb74-a4c14f2664c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.621 | Valid Score: 0.535\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 2/100 | Training Loss: 0.307 | Valid Score: 0.512\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 3/100 | Training Loss: 0.281 | Valid Score: 0.505\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 4/100 | Training Loss: 0.265 | Valid Score: 0.485\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 5/100 | Training Loss: 0.249 | Valid Score: 0.468\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 6/100 | Training Loss: 0.236 | Valid Score: 0.452\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 7/100 | Training Loss: 0.223 | Valid Score: 0.439\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 8/100 | Training Loss: 0.212 | Valid Score: 0.424\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 9/100 | Training Loss: 0.202 | Valid Score: 0.417\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 10/100 | Training Loss: 0.195 | Valid Score: 0.403\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 11/100 | Training Loss: 0.187 | Valid Score: 0.392\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.535 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.535 \n",
            "\n",
            "Test Score: 0.530 \n",
            "\n",
            "Execution time: 9.166 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-31ERbfC8SJB"
      },
      "source": [
        "#GAT 3Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "nZ_xeRRba-c9"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, config, global_size=200, num_tasks=1):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        # Node feature size\n",
        "        self.node_feature_size = self.config.get('node_feature_size', 127)\n",
        "\n",
        "        # Edge feature size\n",
        "        self.edge_feature_size = self.config.get('edge_feature_size', 12)\n",
        "\n",
        "        # Hidden size\n",
        "        self.hidden_size = self.config.get('hidden_size', 100)\n",
        "\n",
        "\n",
        "        # Number of attention heads\n",
        "        self.num_heads = self.config.get('num_heads', 1)\n",
        "\n",
        "        \n",
        "        self.dropout = self.config.get('dropout', 0.0)\n",
        "\n",
        "        # GAT layer1\n",
        "        self.conv1 = GATConv(self.node_feature_size,self.hidden_size,num_heads=self.num_heads,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n",
        "\n",
        "        # Linear layer\n",
        "        self.fc = nn.Linear(self.hidden_size * self.num_heads,self.hidden_size)\n",
        "\n",
        "        # GAT layer2\n",
        "        self.conv2 = GATConv(self.hidden_size,self.hidden_size,num_heads=1,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n",
        "\n",
        "        # GAT layer3\n",
        "        self.conv3 = GATConv(self.hidden_size,self.num_tasks,num_heads=1,feat_drop=self.dropout,attn_drop=self.dropout,allow_zero_in_degree=True)\n",
        "\n",
        "    def forward(self, mol_dgl_graph, globals):\n",
        "        mol_dgl_graph.ndata[\"v\"] = mol_dgl_graph.ndata[\"v\"][:, :self.node_feature_size]\n",
        "        mol_dgl_graph.edata[\"e\"] = mol_dgl_graph.edata[\"e\"][:, :self.edge_feature_size]\n",
        "\n",
        "        # First GAT layer1\n",
        "        h = self.conv1(mol_dgl_graph, mol_dgl_graph.ndata[\"v\"]).flatten(1)\n",
        "        h = F.relu(h)\n",
        "        h = self.fc(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Second GAT layer2\n",
        "        h = self.conv2(mol_dgl_graph, h).squeeze(1)\n",
        "        h = F.relu(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "\n",
        "        # Third GAT layer3\n",
        "        if self.num_tasks == 1:\n",
        "            h = self.conv3(mol_dgl_graph, h).squeeze(1)\n",
        "        else:\n",
        "            hs = []\n",
        "            for i in range(self.num_tasks):\n",
        "                hi = self.conv3(mol_dgl_graph, h).squeeze(1)\n",
        "                hs.append(hi)\n",
        "            h = torch.stack(hs, dim=1)\n",
        "\n",
        "        mol_dgl_graph.ndata[\"h\"] = h\n",
        "\n",
        "        if self.num_tasks == 1:\n",
        "            return dgl.mean_nodes(mol_dgl_graph, \"h\")\n",
        "        else:\n",
        "            return dgl.mean_nodes(mol_dgl_graph, \"h\"), h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "MK6jcIjI8WoQ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "def compute_score(model, data_loader, val_size, num_tasks, scaler=None):\n",
        "  model.eval()\n",
        "  loss_sum = nn.MSELoss(reduction='sum') # MSE with sum instead of mean, i.e., sum_i[(y_i)^2-(y'_i)^2]\n",
        "  final_loss = 0\n",
        "  state = torch.get_rng_state()\n",
        "  with torch.no_grad():\n",
        "            for i, (mol_dgl_graph, labels, masks, globals) in enumerate(data_loader):\n",
        "                prediction = model(mol_dgl_graph, globals)\n",
        "                if scaler is not None:\n",
        "                  prediction = torch.tensor(scaler.inverse_transform(prediction.detach().cpu()))\n",
        "                  labels = torch.tensor(scaler.inverse_transform(labels.cpu()))\n",
        "                loss = loss_sum(prediction, labels)\n",
        "                final_loss += loss.item()\n",
        "            final_loss /= val_size\n",
        "            final_loss = math.sqrt(final_loss) # RMSE\n",
        "  return final_loss / num_tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "SWuSNdee8WxJ"
      },
      "outputs": [],
      "source": [
        "def loss_func(output, label, mask, num_tasks):\n",
        "    pos_weight = torch.ones((1, num_tasks))\n",
        "    pos_weight\n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    loss = mask*criterion(output,label)\n",
        "    loss = loss.sum() / mask.sum()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "OJesGwJE8W6o"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_dataloader, model, optimizer):\n",
        "    epoch_train_loss = 0\n",
        "    iterations = 0\n",
        "    model.train() \n",
        "    for i, (mol_dgl_graph, labels, masks, globals) in enumerate(train_dataloader):\n",
        "        prediction = model(mol_dgl_graph, globals)\n",
        "        loss_train = loss_func(prediction, labels, masks, num_tasks)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "        epoch_train_loss += loss_train.detach().item()\n",
        "        iterations += 1\n",
        "    epoch_train_loss /= iterations\n",
        "    return epoch_train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "j-hNtLC68XCZ"
      },
      "outputs": [],
      "source": [
        "def train_evaluate():\n",
        "\n",
        "    model = GNN(config, global_size, num_tasks)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "    best_val = 0\n",
        "    patience_count = 1\n",
        "    epoch = 1\n",
        "\n",
        "    while epoch <= num_epochs:\n",
        "        if patience_count <= patience:\n",
        "            model.train()\n",
        "            loss_train = train_epoch(train_dataloader, model, optimizer)\n",
        "            model.eval()\n",
        "            score_val = compute_score(model, val_dataloader, len(val_set), num_tasks)\n",
        "            if score_val > best_val:\n",
        "                best_val = score_val\n",
        "                print(\"Save checkpoint\")\n",
        "                path = os.path.join(checkpoint_path, 'checkpoint.pth')\n",
        "                dict_checkpoint = {\"score_val\": score_val}\n",
        "                dict_checkpoint.update({\"model_state_dict\": model.state_dict(), \"optimizer_state\": optimizer.state_dict()})\n",
        "                with open(path, \"wb\") as outputfile:\n",
        "                    cloudpickle.dump(dict_checkpoint, outputfile)\n",
        "                patience_count = 1\n",
        "            else:\n",
        "                print(\"Patience\", patience_count)\n",
        "                patience_count += 1\n",
        "\n",
        "            print(\"Epoch: {}/{} | Training Loss: {:.3f} | Valid Score: {:.3f}\".format(\n",
        "            epoch, num_epochs, loss_train, score_val))\n",
        "\n",
        "            print(\" \")\n",
        "            print(\"Epoch: {}/{} | Best Valid Score Until Now: {:.3f}\".format(epoch, num_epochs, best_val), \"\\n\")\n",
        "        epoch += 1\n",
        "\n",
        "    \n",
        "    shutil.rmtree(best_model_path, ignore_errors=True)\n",
        "    shutil.copytree(checkpoint_path, best_model_path)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"Average Valid Score: {:.3f}\".format(np.mean(best_val)), \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "yvHT16qm8XKq"
      },
      "outputs": [],
      "source": [
        "def test_evaluate():\n",
        "    final_model = GNN(config, global_size, num_tasks)\n",
        "    path = os.path.join(best_model_path, 'checkpoint.pth')\n",
        "    with open(path, 'rb') as f:\n",
        "        checkpoint = cloudpickle.load(f)\n",
        "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    final_model.eval()\n",
        "    test_score = compute_score(final_model, test_dataloader, len(test_set), num_tasks)\n",
        "\n",
        "    print(\"Test Score: {:.3f}\".format(test_score), \"\\n\")\n",
        "    print(\"Execution time: {:.3f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqYi3p6y8XSk",
        "outputId": "3b9d77ea-ab90-4a96-8b44-fcc54fdd64c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Save checkpoint\n",
            "Epoch: 1/100 | Training Loss: 0.352 | Valid Score: 0.464\n",
            " \n",
            "Epoch: 1/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 1\n",
            "Epoch: 2/100 | Training Loss: 0.244 | Valid Score: 0.447\n",
            " \n",
            "Epoch: 2/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 2\n",
            "Epoch: 3/100 | Training Loss: 0.210 | Valid Score: 0.412\n",
            " \n",
            "Epoch: 3/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 3\n",
            "Epoch: 4/100 | Training Loss: 0.191 | Valid Score: 0.389\n",
            " \n",
            "Epoch: 4/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 4\n",
            "Epoch: 5/100 | Training Loss: 0.182 | Valid Score: 0.386\n",
            " \n",
            "Epoch: 5/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 5\n",
            "Epoch: 6/100 | Training Loss: 0.173 | Valid Score: 0.370\n",
            " \n",
            "Epoch: 6/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 6\n",
            "Epoch: 7/100 | Training Loss: 0.165 | Valid Score: 0.359\n",
            " \n",
            "Epoch: 7/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 7\n",
            "Epoch: 8/100 | Training Loss: 0.159 | Valid Score: 0.350\n",
            " \n",
            "Epoch: 8/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 8\n",
            "Epoch: 9/100 | Training Loss: 0.157 | Valid Score: 0.349\n",
            " \n",
            "Epoch: 9/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 9\n",
            "Epoch: 10/100 | Training Loss: 0.153 | Valid Score: 0.346\n",
            " \n",
            "Epoch: 10/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Patience 10\n",
            "Epoch: 11/100 | Training Loss: 0.151 | Valid Score: 0.344\n",
            " \n",
            "Epoch: 11/100 | Best Valid Score Until Now: 0.464 \n",
            "\n",
            "Final results:\n",
            "Average Valid Score: 0.464 \n",
            "\n",
            "Test Score: 0.469 \n",
            "\n",
            "Execution time: 8.935 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "train_evaluate()\n",
        "test_evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
